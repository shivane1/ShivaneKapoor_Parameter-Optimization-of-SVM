{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDp8XAhQ_6KW",
        "outputId": "17ac1838-96c4-4298-9708-3cebd9c95d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Loading Letter Recognition dataset...\n",
            "✅ Loaded dataset with shape (20000, 16) and 26 unique classes.\n",
            "\n",
            "📦 Creating train-test splits...\n",
            "   - Sample 1: Train = 15000, Test = 5000\n",
            "   - Sample 2: Train = 15000, Test = 5000\n",
            "   - Sample 3: Train = 15000, Test = 5000\n",
            "   - Sample 4: Train = 15000, Test = 5000\n",
            "   - Sample 5: Train = 15000, Test = 5000\n",
            "   - Sample 6: Train = 15000, Test = 5000\n",
            "   - Sample 7: Train = 15000, Test = 5000\n",
            "   - Sample 8: Train = 15000, Test = 5000\n",
            "   - Sample 9: Train = 15000, Test = 5000\n",
            "   - Sample 10: Train = 15000, Test = 5000\n",
            "\n",
            "🚀 Optimizing SVM for Sample 1...\n",
            "   🧠 Best Parameters: {'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
            "   📈 Test Accuracy: 0.9652\n",
            "   ⏱️ Time Taken: 123.28 seconds\n",
            "\n",
            "🚀 Optimizing SVM for Sample 2...\n",
            "   🧠 Best Parameters: {'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
            "   📈 Test Accuracy: 0.9724\n",
            "   ⏱️ Time Taken: 130.30 seconds\n",
            "\n",
            "🚀 Optimizing SVM for Sample 3...\n",
            "   🧠 Best Parameters: {'kernel': 'linear', 'gamma': 'auto', 'C': 5}\n",
            "   📈 Test Accuracy: 0.8546\n",
            "   ⏱️ Time Taken: 126.04 seconds\n",
            "\n",
            "🚀 Optimizing SVM for Sample 4...\n",
            "   🧠 Best Parameters: {'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
            "   📈 Test Accuracy: 0.9702\n",
            "   ⏱️ Time Taken: 102.91 seconds\n",
            "\n",
            "🚀 Optimizing SVM for Sample 5...\n",
            "   🧠 Best Parameters: {'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
            "   📈 Test Accuracy: 0.9684\n",
            "   ⏱️ Time Taken: 101.74 seconds\n",
            "\n",
            "🚀 Optimizing SVM for Sample 6...\n",
            "   🧠 Best Parameters: {'kernel': 'rbf', 'gamma': 'auto', 'C': 1}\n",
            "   📈 Test Accuracy: 0.9430\n",
            "   ⏱️ Time Taken: 122.44 seconds\n",
            "\n",
            "🚀 Optimizing SVM for Sample 7...\n",
            "   🧠 Best Parameters: {'kernel': 'rbf', 'gamma': 'auto', 'C': 5}\n",
            "   📈 Test Accuracy: 0.9688\n",
            "   ⏱️ Time Taken: 119.27 seconds\n",
            "\n",
            "🚀 Optimizing SVM for Sample 8...\n",
            "   🧠 Best Parameters: {'kernel': 'rbf', 'gamma': 'auto', 'C': 1}\n",
            "   📈 Test Accuracy: 0.9398\n",
            "   ⏱️ Time Taken: 102.22 seconds\n",
            "\n",
            "🚀 Optimizing SVM for Sample 9...\n",
            "   🧠 Best Parameters: {'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
            "   📈 Test Accuracy: 0.9752\n",
            "   ⏱️ Time Taken: 107.06 seconds\n",
            "\n",
            "🚀 Optimizing SVM for Sample 10...\n",
            "   🧠 Best Parameters: {'kernel': 'rbf', 'gamma': 'scale', 'C': 1}\n",
            "   📈 Test Accuracy: 0.9446\n",
            "   ⏱️ Time Taken: 139.24 seconds\n",
            "\n",
            "📁 Results table saved to 'svm_results_20250419_172148.csv'\n",
            "      Sample Accuracy                Parameters\n",
            "0   Sample 1   0.9652   K: rbf, C: 10, γ: scale\n",
            "1   Sample 2   0.9724    K: rbf, C: 10, γ: auto\n",
            "2   Sample 3   0.8546  K: linear, C: 5, γ: auto\n",
            "3   Sample 4   0.9702    K: rbf, C: 10, γ: auto\n",
            "4   Sample 5   0.9684   K: rbf, C: 10, γ: scale\n",
            "5   Sample 6   0.9430     K: rbf, C: 1, γ: auto\n",
            "6   Sample 7   0.9688     K: rbf, C: 5, γ: auto\n",
            "7   Sample 8   0.9398     K: rbf, C: 1, γ: auto\n",
            "8   Sample 9   0.9752    K: rbf, C: 10, γ: auto\n",
            "9  Sample 10   0.9446    K: rbf, C: 1, γ: scale\n",
            "\n",
            "📊 Convergence graph saved as: convergence_plot_sample9_20250419_172148.png\n",
            "\n",
            "📋 Analytics summary saved to 'analytics_summary_20250419_172148.txt'\n",
            "\n",
            "✅ All tasks completed.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "import warnings\n",
        "import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    print(\"🔍 Loading Letter Recognition dataset...\")\n",
        "    X, y = fetch_openml(name='letter', version=1, return_X_y=True, as_frame=False)\n",
        "    print(f\"✅ Loaded dataset with shape {X.shape} and {len(np.unique(y))} unique classes.\")\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def create_samples(X, y, n_samples=10):\n",
        "    samples = []\n",
        "    print(\"\\n📦 Creating train-test splits...\")\n",
        "    for i in range(n_samples):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.25, random_state=i*10\n",
        "        )\n",
        "        print(f\"   - Sample {i+1}: Train = {X_train.shape[0]}, Test = {X_test.shape[0]}\")\n",
        "        samples.append((X_train, X_test, y_train, y_test))\n",
        "    return samples\n",
        "\n",
        "\n",
        "def optimize_svm(X_train, X_test, y_train, y_test, sample_num):\n",
        "    print(f\"\\n🚀 Optimizing SVM for Sample {sample_num}...\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    param_grid = {\n",
        "        'kernel': ['rbf', 'linear', 'sigmoid'],\n",
        "        'C': [0.1, 1, 5, 10],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }\n",
        "\n",
        "    svm = SVC()\n",
        "    random_search = RandomizedSearchCV(\n",
        "        svm, param_grid,\n",
        "        n_iter=8,\n",
        "        cv=3,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1,\n",
        "        random_state=sample_num * 100,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    accuracies = []\n",
        "    start_time = time.time()\n",
        "    random_search.fit(X_train_scaled, y_train)\n",
        "    scores = random_search.cv_results_['mean_test_score']\n",
        "    sorted_scores = sorted(scores)\n",
        "    best_scores = []\n",
        "    best_so_far = 0\n",
        "\n",
        "    for score in sorted_scores:\n",
        "        best_so_far = max(best_so_far, score)\n",
        "        best_scores.append(best_so_far)\n",
        "\n",
        "    interp_points = np.linspace(0, len(best_scores)-1, 100)\n",
        "    interp_scores = np.interp(interp_points, np.arange(len(best_scores)), best_scores)\n",
        "    accuracies = list(interp_scores)\n",
        "\n",
        "    best_params = random_search.best_params_\n",
        "    best_model = random_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test_scaled)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"   🧠 Best Parameters: {best_params}\")\n",
        "    print(f\"   📈 Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"   ⏱️ Time Taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    return {\n",
        "        'sample': sample_num,\n",
        "        'best_accuracy': test_accuracy,\n",
        "        'best_params': best_params,\n",
        "        'kernel': best_params['kernel'],\n",
        "        'C': best_params['C'],\n",
        "        'gamma': best_params['gamma'],\n",
        "        'convergence': accuracies\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_convergence(results, best_sample_idx):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(results[best_sample_idx]['convergence'], marker='o', linestyle='--', color='navy')\n",
        "    plt.title(f\"Convergence for Sample {best_sample_idx + 1}\", fontsize=14)\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True)\n",
        "    filename = f\"convergence_plot_sample{best_sample_idx+1}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "    print(f\"\\n📊 Convergence graph saved as: {filename}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    X, y = load_dataset()\n",
        "    samples = create_samples(X, y)\n",
        "    results = []\n",
        "\n",
        "    for i, (X_train, X_test, y_train, y_test) in enumerate(samples):\n",
        "        result = optimize_svm(X_train, X_test, y_train, y_test, i+1)\n",
        "        results.append(result)\n",
        "\n",
        "    best_sample_idx = np.argmax([r['best_accuracy'] for r in results])\n",
        "    table_data = [{\n",
        "        'Sample': f\"Sample {i+1}\",\n",
        "        'Accuracy': f\"{r['best_accuracy']:.4f}\",\n",
        "        'Parameters': f\"K: {r['kernel']}, C: {r['C']}, γ: {r['gamma']}\"\n",
        "    } for i, r in enumerate(results)]\n",
        "\n",
        "    results_df = pd.DataFrame(table_data)\n",
        "    results_filename = f\"svm_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "    results_df.to_csv(results_filename, index=False)\n",
        "    print(f\"\\n📁 Results table saved to '{results_filename}'\")\n",
        "    print(results_df)\n",
        "\n",
        "    plot_convergence(results, best_sample_idx)\n",
        "\n",
        "    analytics_data = {\n",
        "        'dataset_name': 'Letter Recognition',\n",
        "        'dataset_size': X.shape,\n",
        "        'test_size_per_sample': int(X.shape[0] * 0.25),\n",
        "        'num_classes': len(np.unique(y)),\n",
        "        'best_sample_number': best_sample_idx + 1,\n",
        "        'best_accuracy': results[best_sample_idx]['best_accuracy'],\n",
        "        'best_params': results[best_sample_idx]['best_params']\n",
        "    }\n",
        "\n",
        "    analytics_filename = f\"analytics_summary_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "    with open(analytics_filename, 'w') as f:\n",
        "        for key, value in analytics_data.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    print(f\"\\n📋 Analytics summary saved to '{analytics_filename}'\")\n",
        "    print(\"\\n✅ All tasks completed.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aYNih5TP_8hg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}